{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import openpyxl\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D:\\\\all_CTI_report\"\n",
    "file_path={}\n",
    "for res in os.walk(path):\n",
    "    if \"\\\\txt\" == res[0][-4:]:\n",
    "        p=res[0]\n",
    "        source=p.replace(\"D:\\\\all_CTI_report\\\\\", \"\").replace(\"\\\\txt\", \"\").replace(\"\\\\\", \"_\")\n",
    "        file_path[source]=p\n",
    "sources=list(file_path.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def sent2token(sentence):\n",
    "    sw=stopwords.words('english')+[i for i in string.hexdigits+string.punctuation]\n",
    "    tokens = nltk.word_tokenize(sentence)  \n",
    "    tagged_sent = nltk.pos_tag(tokens)     \n",
    "\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or nltk.corpus.wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos).lower())\n",
    "        token=[i for i in lemmas_sent if i not in sw]\n",
    "    return token,lemmas_sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2token(path, source, txt_name, path_):\n",
    "    for j in txt_name:\n",
    "        tokens=[]\n",
    "        tokens_clean=[]\n",
    "        with open(path_+\"\\\\\"+j, 'rb') as f:\n",
    "            byte_txt=f.read()\n",
    "        txt=byte_txt.decode(\"utf-8\")\n",
    "        txt=txt.replace(\"\\n\", \" \")\n",
    "        sent=nltk.sent_tokenize(txt)\n",
    "        for k in sent:\n",
    "            token_clean, token=sent2token(k)\n",
    "            tokens=tokens+token\n",
    "            tokens_clean=tokens_clean+token_clean\n",
    "        info=[j, source, len(sent), len(token), len(token_clean)] #, token, sent\n",
    "        wb = openpyxl.load_workbook(path + '\\\\CTI_nltk.xlsx', data_only=True)\n",
    "        s1 = wb.active\n",
    "        s1.append(info)\n",
    "        # s2= wb[source]\n",
    "        # s2.append(info)\n",
    "        wb.save(path + '\\\\CTI_nltk.xlsx')\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_str = ['txt_name', 'source', '# of sentence', '# of word'] #, 'words', 'sentences'\n",
    "\n",
    "# with open(path + \"\\\\CTI_nltk.csv\", 'w', newline='', encoding='UTF-8') as f:\n",
    "#     csv_writer = csv.writer(f)\n",
    "#     csv_writer.writerow(info_str)\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "s1 = wb.active\n",
    "s1.append(info_str)\n",
    "wb.save(path + '\\\\CTI_nltk.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APTnotes : 897.3435273170471 s\n"
     ]
    }
   ],
   "source": [
    "path=\"D:\\\\all_CTI_report\"\n",
    "info_str = ['txt_name', 'source', '# of sentence', '# of word', '# of word(w/o stopwords)'] # , 'words', 'sentences'\n",
    "wb = openpyxl.Workbook()\n",
    "s1=wb.active\n",
    "s1.append(info_str)\n",
    "wb.save(path + '\\\\CTI_nltk_APT.xlsx')\n",
    "file_path={}\n",
    "error_list=[]\n",
    "for res in os.walk(path):\n",
    "    if \"\\\\txt\" == res[0][-4:]:\n",
    "        p=res[0]\n",
    "        source=p.replace(\"D:\\\\all_CTI_report\\\\\", \"\").replace(\"\\\\txt\", \"\").replace(\"\\\\\", \"_\")\n",
    "        file_path[source]=p\n",
    "sources=list(file_path.keys())\n",
    "for i in sources: \n",
    "    start = time.time()\n",
    "    path_=file_path[i]\n",
    "    txt_name=os.listdir(path_)\n",
    "    for j in txt_name:\n",
    "        tokens=[]\n",
    "        tokens_clean=[]\n",
    "        try:\n",
    "            with open(path_+\"\\\\\"+j, 'rb') as f:\n",
    "                byte_txt=f.read()\n",
    "            txt=byte_txt.decode(\"utf-8\")\n",
    "        except:\n",
    "            error_list.append(path_+\"\\\\\"+j)\n",
    "            continue\n",
    "\n",
    "        txt=txt.replace(\"\\n\", \" \")\n",
    "        sent=nltk.sent_tokenize(txt)\n",
    "        for k in sent:\n",
    "            token_clean, token=sent2token(k)\n",
    "            tokens=tokens+token\n",
    "            tokens_clean=tokens_clean+token_clean\n",
    "        info=[j, i, len(sent), len(tokens), len(tokens_clean)]\n",
    "        wb = openpyxl.load_workbook(path + '\\\\CTI_nltk_APT.xlsx', data_only=True)\n",
    "        s1 = wb.active\n",
    "        s1.append(info)\n",
    "        wb.save(path + '\\\\CTI_nltk_APT.xlsx')\n",
    "    end = time.time()\n",
    "    print(i,\":\", end - start, \"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
